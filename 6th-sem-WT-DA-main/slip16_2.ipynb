{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "060a6cec-7e4b-40b7-9441-44d0e040d395",
   "metadata": {},
   "source": [
    "Q. 2)Consider any text paragraph. Preprocess the text to remove any special characters and digits. \n",
    "Generate the summary using extractive summarization process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9633fa8-9377-4015-ac08-8bb696d19a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNatural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between \\ncomputers and human language, in particular how to program computers to process and analyze large amounts of natural language data. Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize , sent_tokenize\n",
    "import re\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters and digits\n",
    "    processed_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    processed_text = re.sub(r'\\d+', '', processed_text)\n",
    "    return processed_text\n",
    "\n",
    "# Example text paragraph\n",
    "text = \"\"\"\n",
    "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between \n",
    "computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. \n",
    "The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. \n",
    "The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves. \n",
    "Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.\n",
    "\"\"\"\n",
    "\n",
    "# Preprocess the text\n",
    "formated_text = preprocess_text(text)\n",
    "\n",
    "stopWords = set(stopwords.words(\"english\"))\n",
    "words = word_tokenize(formated_text)\n",
    "\n",
    "wordfreq = {}\n",
    "for word in words:\n",
    "    if word in stopWords:\n",
    "        continue\n",
    "    if word in wordfreq:\n",
    "        wordfreq[word]+=1\n",
    "    else:\n",
    "        wordfreq[word] =1\n",
    "\n",
    "maximum_freq = max(wordfreq.values())\n",
    "\n",
    "for word in wordfreq.keys():\n",
    "    wordfreq[word] = (wordfreq[word]/maximum_freq)\n",
    "\n",
    "sentences  = sent_tokenize(text)\n",
    "sentencesValue = {}\n",
    "for sentence in sentences:\n",
    "    for word,frq in wordfreq.items():\n",
    "        if word in sentence.lower():\n",
    "            if sentence in sentencesValue:\n",
    "                sentencesValue[sentence] += frq\n",
    "            else:\n",
    "                sentencesValue[sentence] = frq\n",
    "\n",
    "\n",
    "import heapq \n",
    "summary = '' \n",
    "summary_sentence = heapq.nlargest(10,sentencesValue , key = sentencesValue.get)\n",
    "summary = ' '.join(summary_sentence)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d959b9cf-fd2b-4ce2-b42b-122c4fd1fca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\n",
      "The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them.\n",
      "The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\n",
      "Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.\n"
     ]
    }
   ],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "\n",
    "# Example text paragraph\n",
    "text = \"\"\"\n",
    "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between \n",
    "computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. \n",
    "The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. \n",
    "The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves. \n",
    "Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.\n",
    "\"\"\"\n",
    "# Initialize parser and tokenizer\n",
    "parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "\n",
    "# Initialize LexRank summarizer\n",
    "summarizer = LexRankSummarizer()\n",
    "# Generate summary\n",
    "summary = summarizer(parser.document, sentences_count=4)  # Change the sentences_count parameter to adjust summary length\n",
    "\n",
    "# Print summary\n",
    "for sentence in summary:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb70506-cc95-4a50-8f49-a5cca6cd153e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
